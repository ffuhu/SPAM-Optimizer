[32m2025-01-14 11:58:23.182[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m209[0m - [1mUsing dist with rank 0 (only rank 0 will log)[0m
[32m2025-01-14 11:58:23.182[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m210[0m - [1m****************************************[0m
[32m2025-01-14 11:58:23.182[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m211[0m - [1mStarting training with the arguments[0m
[32m2025-01-14 11:58:23.182[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_config                   configs/llama_60m.json[0m
model_config                   configs/llama_60m.json
[32m2025-01-14 11:58:23.182[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1muse_hf_model                   False[0m
use_hf_model                   False
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mcontinue_from                  None[0m
continue_from                  None
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbatch_size                     128[0m
batch_size                     128
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgradient_accumulation          4[0m
gradient_accumulation          4
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtotal_batch_size               512[0m
total_batch_size               512
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_length                     256[0m
max_length                     256
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1moptimizer                      SPAM[0m
optimizer                      SPAM
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mlr                             0.001[0m
lr                             0.001
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mscheduler                      cosine[0m
scheduler                      cosine
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmin_lr_ratio                   0.1[0m
min_lr_ratio                   0.1
[32m2025-01-14 11:58:23.183[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mactivation_checkpointing       False[0m
activation_checkpointing       False
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mweight_decay                   0.0[0m
weight_decay                   0.0
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_steps                   1000[0m
warmup_steps                   1000
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1meval_every                     1000[0m
eval_every                     1000
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mnum_training_steps             10000[0m
num_training_steps             10000
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmax_train_tokens               None[0m
max_train_tokens               None
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_every                     2000[0m
save_every                     2000
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msave_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469[0m
save_dir                       /scratch-shared/HTJ2/checkpoints/new0_9390469
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mtags                           None[0m
tags                           None
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdtype                          bfloat16[0m
dtype                          bfloat16
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mworkers                        8[0m
workers                        8
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mseed                           0[0m
seed                           0
[32m2025-01-14 11:58:23.184[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mname                           test[0m
name                           test
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mmodel_type                     llama[0m
model_type                     llama
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_clipping                  0.0[0m
grad_clipping                  0.0
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mbeta1                          0.0[0m
beta1                          0.0
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgalore_scale                   1.0[0m
galore_scale                   1.0
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1msingle_gpu                     False[0m
single_gpu                     False
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mload_local                     False[0m
load_local                     False
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mwarmup_epoch                   150[0m
warmup_epoch                   150
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mthreshold                      5000.0[0m
threshold                      5000.0
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mgrad_accu_steps                20[0m
grad_accu_steps                20
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mdensity                        1.0[0m
density                        1.0
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m213[0m - [1mupdate_gap                     500[0m
update_gap                     500
[32m2025-01-14 11:58:23.185[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m215[0m - [1m****************************************[0m
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:02<00:00, 384.62it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 471922.57it/s]
[32m2025-01-14 11:58:30.900[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m228[0m - [1mShuffling data with seed 42[0m
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/10000 [00:00<?, ?it/s][32m2025-01-14 11:58:32.478[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m345[0m - [1m
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 512, padding_idx=31999)
    (layers): ModuleList(
      (0-7): 8 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=512, out_features=512, bias=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=False)
          (v_proj): Linear(in_features=512, out_features=512, bias=False)
          (o_proj): Linear(in_features=512, out_features=512, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=512, out_features=1376, bias=False)
          (down_proj): Linear(in_features=1376, out_features=512, bias=False)
          (up_proj): Linear(in_features=512, out_features=1376, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=512, out_features=32000, bias=False)
)
[0m
[32m2025-01-14 11:58:32.478[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m346[0m - [1mTotal params: 58.07M[0m
[32m2025-01-14 11:58:32.478[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m347[0m - [1mTrainable params: 58.07M[0m
[32m2025-01-14 11:58:32.479[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m350[0m - [1mSaving model to /scratch-shared/HTJ2/checkpoints/new0_9390469 every 2000 update steps[0m
/gpfs/home2/huangti/SPAM_v2/galore_torch/SPAM.py:104: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use `torch.optim.AdamW` instead, or set `no_deprecation_warning=True` to disable this warning.
  warnings.warn(
density 1.0
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at /opt/conda/conda-bld/pytorch_1728945379270/work/aten/src/ATen/native/cudnn/MHA.cpp:674.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass

loss at 4 2.625
Update steps:   5%|â–ˆâ–Ž                       | 500/10000 [03:04<55:42,  2.84it/s]/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate

loss at 404 1.90625

loss at 804 1.75

loss at 1204 1.5703125

loss at 1604 1.453125
lr 0.000498
lr 0.000498
Mask overlap ratio: 1.00
Mask Update
  warnings.warn(
/home/huangti/miniconda3/envs/spam/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

loss at 2004 1.3515625
Update steps:  10%|â–ˆâ–ˆâ–                     | 1000/10000 [06:02<52:26,  2.86it/s][32m2025-01-14 12:04:34.917[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m496[0m - [1mPerforming evaluation at step 1000[0m

loss at 2404 1.3125

loss at 2804 1.2109375

loss at 3204 1.1484375

loss at 3604 1.1171875
lr 0.000998
lr 0.000998
Mask overlap ratio: 1.00
Mask Update
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:01<00:00, 559.00it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:01<00:00, 678.75it/s]
[32m2025-01-14 12:04:41.800[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m105[0m - [1mLoaded validation dataset in 6.88 seconds[0m
[32m2025-01-14 12:04:41.802[0m | [1mINFO    [0m | [36m__main__[0m:[36mevaluate_model[0m:[36m121[0m - [1mEval set prepared in 6.88 seconds[0m
[32m2025-01-14 12:05:16.620[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m508[0m - [1mEval loss at step 1000: 4.284367561340332,tokens_seen: 99979486[0m

loss at 4004 1.0703125
Update steps:  12%|â–ˆâ–ˆâ–Š                     | 1173/10000 [07:43<50:20,  2.92it/s]

loss at 4404 1.0546875
